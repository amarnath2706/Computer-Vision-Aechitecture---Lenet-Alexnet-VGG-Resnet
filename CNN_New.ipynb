{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 images belonging to 3 classes.\n",
      "Found 37 images belonging to 3 classes.\n",
      "Epoch 1/30\n",
      "2/2 [==============================] - 3s 2s/step - loss: 1.1763 - accuracy: 0.3243 - val_loss: 0.9053 - val_accuracy: 0.4054\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 2s 901ms/step - loss: 1.4428 - accuracy: 0.3784 - val_loss: 1.1941 - val_accuracy: 0.3784\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 2s 885ms/step - loss: 1.3143 - accuracy: 0.3784 - val_loss: 1.0865 - val_accuracy: 0.4595\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 2s 917ms/step - loss: 1.0154 - accuracy: 0.4324 - val_loss: 1.3303 - val_accuracy: 0.4054\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 2s 941ms/step - loss: 1.0652 - accuracy: 0.3514 - val_loss: 1.0671 - val_accuracy: 0.3784\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 2s 955ms/step - loss: 1.1134 - accuracy: 0.3514 - val_loss: 1.2347 - val_accuracy: 0.3514\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 2s 914ms/step - loss: 1.0956 - accuracy: 0.3784 - val_loss: 1.0923 - val_accuracy: 0.4054\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 2s 952ms/step - loss: 1.0615 - accuracy: 0.3784 - val_loss: 1.0083 - val_accuracy: 0.6486\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 2s 932ms/step - loss: 1.0062 - accuracy: 0.6486 - val_loss: 0.9428 - val_accuracy: 0.6486\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 2s 988ms/step - loss: 1.0246 - accuracy: 0.5405 - val_loss: 0.8552 - val_accuracy: 0.6486\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 2s 929ms/step - loss: 0.9958 - accuracy: 0.5946 - val_loss: 0.9214 - val_accuracy: 0.6757\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 2s 921ms/step - loss: 0.9063 - accuracy: 0.5946 - val_loss: 0.8210 - val_accuracy: 0.6757\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 2s 874ms/step - loss: 0.8979 - accuracy: 0.6486 - val_loss: 0.9472 - val_accuracy: 0.7027\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.7268 - accuracy: 0.7297 - val_loss: 0.8219 - val_accuracy: 0.7297\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6800 - accuracy: 0.7568 - val_loss: 0.4326 - val_accuracy: 0.7027\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 2s 971ms/step - loss: 0.7591 - accuracy: 0.6757 - val_loss: 0.5875 - val_accuracy: 0.7297\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 2s 918ms/step - loss: 0.7523 - accuracy: 0.7297 - val_loss: 0.5843 - val_accuracy: 0.7297\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 2s 930ms/step - loss: 0.7158 - accuracy: 0.6216 - val_loss: 0.4114 - val_accuracy: 0.5676\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 2s 888ms/step - loss: 0.8615 - accuracy: 0.5946 - val_loss: 0.4962 - val_accuracy: 0.8108\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.5491 - accuracy: 0.7297 - val_loss: 0.8068 - val_accuracy: 0.7297\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 2s 928ms/step - loss: 0.6294 - accuracy: 0.7297 - val_loss: 0.5619 - val_accuracy: 0.7568\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 2s 929ms/step - loss: 0.6003 - accuracy: 0.7568 - val_loss: 0.2451 - val_accuracy: 0.8919\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 2s 879ms/step - loss: 0.5170 - accuracy: 0.8919 - val_loss: 0.6575 - val_accuracy: 0.8378\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 2s 983ms/step - loss: 0.4317 - accuracy: 0.8919 - val_loss: 0.2876 - val_accuracy: 0.7568\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 2s 934ms/step - loss: 0.5149 - accuracy: 0.8108 - val_loss: 0.6266 - val_accuracy: 0.8649\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 2s 924ms/step - loss: 0.3412 - accuracy: 0.8649 - val_loss: 0.5038 - val_accuracy: 0.8378\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 2s 920ms/step - loss: 0.3713 - accuracy: 0.8919 - val_loss: 0.1156 - val_accuracy: 0.8649\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 2s 907ms/step - loss: 0.3673 - accuracy: 0.8649 - val_loss: 0.3707 - val_accuracy: 0.8649\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 2s 901ms/step - loss: 0.2940 - accuracy: 0.8919 - val_loss: 0.2473 - val_accuracy: 0.9189\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 2s 949ms/step - loss: 0.3584 - accuracy: 0.8919 - val_loss: 0.6045 - val_accuracy: 0.8649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1588b26b38>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('F:/Dataset/CNN/train',\n",
    "                                                target_size = (64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory('F:/Dataset/CNN/test',\n",
    "                                                target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='categorical')\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch=len(training_set),\n",
    "                         epochs=30,\n",
    "                         validation_data=test_set,\n",
    "                         validation_steps=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n",
      "Apj\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('F:/Dataset/CNN/A1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Apj'\n",
    "    print(prediction)\n",
    "elif result[0][1] == 1:\n",
    "    prediction = 'Dhoni'\n",
    "    print(prediction)\n",
    "    \n",
    "else:\n",
    "    prediction = 'Sachin'\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
